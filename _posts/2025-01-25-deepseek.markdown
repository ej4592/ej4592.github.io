---
layout: post
title: "Paper Review: DeepSeek-R1"
date: 2025-01-25 21:00:20 -0500 # Adjust to a date before today
description: A review of DeepSeek-R1, an RL-trained reasoning LLM that improves self-evolution in problem-solving.
img: "deepseek.avif" # Add image if needed
fig-caption: 
tags: [AI, Paper Review, Reinforcement Learning, Math]
mathjax: true
---

## **Overview**
Recent work on Large Language Models (LLMs) has focused on **reinforcement learning (RL) for self-improvement**. While OpenAIâ€™s o1 models enhanced **reasoning abilities** using inference-time scaling and chain-of-thought prompting, reproducing this performance in **open-source models** has remained a challenge.

DeepSeek-R1 explores **pure RL-based reasoning** using the **GRPO algorithm**, improving math and logic tasks **without supervised fine-tuning (SFT)**. It achieves:
- **71.0% Pass@1 on AIME 2024** (vs. 15.6% before RL, up to 86.7% with majority voting).
- **Performance parity with OpenAI-o1-0912 and o1-1217 in reasoning**.
- **Superior distillation capabilities**, improving smaller models such as Qwen2.5-32B and Llama.

---

## **Reinforcement Learning Algorithm: GRPO**
DeepSeek-R1 uses **Group Relative Policy Optimization (GRPO)**, a modification of **PPO** that eliminates the need for a critic network.

### **Policy Optimization Objective**
Given a set of outputs $ \{o_1, o_2, \dots, o_G\} $ sampled from the old policy $ \pi_{\theta_{\text{old}}} $, the GRPO objective is:
$$
J_{\mathrm{GRPO}}(\theta) = \mathbb{E}_{q \sim P(Q),\, \{o_i\} \sim \pi_{\theta_{\text{old}}}} \left[
\frac{1}{G} \sum_{i=1}^{G} \min \left(
\frac{\pi_{\theta}(o_i \mid q)}{\pi_{\theta_{\text{old}}}(o_i \mid q)}\, A_i,\;
\operatorname{clip}\!\left( \frac{\pi_{\theta}(o_i \mid q)}{\pi_{\theta_{\text{old}}}(o_i \mid q)},\, 1-\epsilon,\, 1+\epsilon \right) A_i
\right)
\mathbin{-}\beta\, D_{\mathrm{KL}}\!\left(\pi_{\theta}\,\Vert\,\pi_{\text{ref}}\right)
\right]
$$

where:
- $ A_i $ is the **advantage function** based on group reward normalization:
  $$
  A_i = \frac{r_i - \operatorname{mean}\bigl(\{r_1, \dots, r_G\}\bigr)}{\operatorname{std}\bigl(\{r_1, \dots, r_G\}\bigr)}
  $$
- $ D_{\mathrm{KL}}\left(\pi_{\theta}\,\Vert\,\pi_{\text{ref}}\right) $ **regularizes policy updates**, preventing collapse.

---

## **Reward Modeling**
DeepSeek-R1 **avoids reward hacking** using structured, rule-based rewards:
1. **Accuracy Reward:** Math/coding correctness is evaluated via test cases.
2. **Format Reward:** The model is required to produce its output in a structured format, ensuring a clear separation between its internal reasoning process and the final answer.

---

## **Emergent Self-Evolution & Benchmark Gains**
During training, DeepSeek-R1 exhibits a self-evolution process by:
- Increasing its reasoning steps and thinking time.
- Developing emergent behaviors such as self-reflection and multi-solution exploration.

Key benchmark improvements include:
- **AIME 2024 (Pass@1):** Improvement from 15.6% to 71.0% (up to 86.7% with majority voting).
- **MATH-500 (Pass@1):** Achieving 97.3%, comparable to leading models.
- **GPQA Diamond Tasks:** Reaching 71.5% accuracy.

---

## **Multi-Stage Training Strategy**
DeepSeek-R1 utilizes a multi-stage training pipeline:

### **Cold-Start Phase**
A small amount of high-quality, long Chain-of-Thought (CoT) data is used to fine-tune the base model. This phase:
- Enhances readability and output alignment.
- Structures responses in a format similar to:


### **Reinforcement Learning Phase**
After the cold-start phase, iterative RL training is applied to further improve reasoning abilities, particularly in math and coding tasks. This phase leverages:
- **Rule-based rewards** for accuracy and language consistency.
- Incentives to ensure clear, structured chain-of-thought outputs.

---

## **Distillation: Transferring Reasoning to Smaller Models**
DeepSeek-R1's learned reasoning patterns are distilled into smaller dense models (e.g., Qwen2.5-32B, Llama variants). This process demonstrates that:
- Distilled models outperform smaller models trained solely via RL.
- The reasoning capabilities of large models can be effectively transferred, preserving high performance.

A comparison is provided below:

| **Model**               | **AIME 2024 Pass@1** | **MATH-500 Pass@1** | **GPQA Diamond Pass@1** |
|-------------------------|----------------------|---------------------|-------------------------|
| RL on Qwen-32B          | 47.0\%              | 91.6\%              | 55.0\%                 |
| **Distilled Qwen-32B**  | **72.6\%**          | **94.3\%**          | **62.1\%**             |

---

## **Challenges and Limitations**
Key challenges identified include:
- **Language Mixing:** Occasional mixing of languages in outputs.
- **Prompt Sensitivity:** Few-shot prompting can degrade performance.
- **Software Engineering Tasks:** RL is less efficient for long-horizon tasks such as full-stack coding.

---

## **Conclusion and Future Work**
DeepSeek-R1 demonstrates that LLMs can develop advanced reasoning capabilities through pure RL, without relying on supervised fine-tuning. The integrated pipeline of cold-start fine-tuning, iterative RL training, and subsequent distillation yields state-of-the-art performance on reasoning benchmarks.

Future research directions include:
- Enhancing **multilingual reasoning** consistency.
- Developing more robust **few-shot prompting** strategies.
- Optimizing RL for **long-horizon tasks** in software engineering.

