<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>Paper Review: DeepSeek-R1 - Hello, world! I'm Eric Ji</title>

  <!-- Edit site and author settings in `_config.yml` to make the social details your own -->

    <meta content="Hello, world! I'm Eric Ji" property="og:site_name">
  
    <meta content="Paper Review: DeepSeek-R1" property="og:title">
  
  
    <meta content="article" property="og:type">
  
  
    <meta content="A review of DeepSeek-R1, an RL-trained reasoning LLM that improves self-evolution in problem-solving." property="og:description">
  
  
    <meta content="http://localhost:4000/deepseek/" property="og:url">
  
  
    <meta content="2025-01-25T21:00:20-05:00" property="article:published_time">
    <meta content="http://localhost:4000/about/" property="article:author">
  
  
    <meta content="http://localhost:4000/assets/img/deepseek.avif" property="og:image">
  
  
    
  
  
    
    <meta content="AI" property="article:tag">
    
    <meta content="Paper Review" property="article:tag">
    
    <meta content="Reinforcement Learning" property="article:tag">
    
    <meta content="Math" property="article:tag">
    
  

    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@">
    <meta name="twitter:creator" content="@">
  
    <meta name="twitter:title" content="Paper Review: DeepSeek-R1">
  
  
    <meta name="twitter:url" content="http://localhost:4000/deepseek/">
  
  
    <meta name="twitter:description" content="A review of DeepSeek-R1, an RL-trained reasoning LLM that improves self-evolution in problem-solving.">
  
  
    <meta name="twitter:image:src" content="http://localhost:4000/assets/img/deepseek.avif">
  

	<meta name="description" content="A review of DeepSeek-R1, an RL-trained reasoning LLM that improves self-evolution in problem-solving.">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta property="og:image" content="">
	<link rel="shortcut icon" href="/assets/img/favicon/favicon.ico" type="image/x-icon">
	<link rel="apple-touch-icon" href="/assets/img/favicon/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicon/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicon/apple-touch-icon-144x144.png">
	<!-- Chrome, Firefox OS and Opera -->
	<meta name="theme-color" content="#263959">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#263959">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#263959">
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=PT+Serif:400,700|Lato:300,400,700&display=swap" rel="stylesheet">
	<!-- Font Awesome -->
	<link rel="stylesheet" href="/assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="/assets/css/main.css">
</head>

<body>

  <div class="wrapper">
    <aside class="sidebar">
  <header>
    <div class="about">
      <div class="cover-author-image">
        <a href="/"><img src="/assets/img/eric-headshot.jpg" alt="Eric Ji"></a>
      </div>
      <div class="author-name">Eric Ji</div>
      <p>Feeling self important enough to share my opinions online</p>
    </div>
  </header> <!-- End Header -->
  <footer>
    <section class="contact">
      <h3 class="contact-title">Contact me</h3>
      <ul>
        
          <li><a href="https://twitter.com/artemsheludko_" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a></li>
        
        
          <li><a href="https://facebook.com/" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a></li>
        
        
          <li class="github"><a href="http://github.com/ej4592" target="_blank"><i class="fa fa-github"></i></a></li>
        
        
          <li class="linkedin"><a href="https://in.linkedin.com/" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a></li>
        
        
          <li class="email"><a href="mailto:ericji300@gmail.com"><i class="fa fa-envelope-o"></i></a></li>
        
      </ul>
    </section> <!-- End Section Contact -->
    <div class="copyright">
      <p>2025 &copy; Eric Ji</p>
    </div>
  </footer> <!-- End Footer -->
</aside> <!-- End Sidebar -->
<div class="content-box clearfix">
  
<!-- MathJax Configuration -->
<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<!-- MathJax Script -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<article class="article-page">
  <div class="page-content">
    
    <div class="page-cover-image">
      <figure>
        <img class="page-image" src="/assets/img/deepseek.avif"
          alt="Paper Review: DeepSeek-R1">
        
      </figure>
    </div> <!-- End Page Cover Image -->
    
    <div class="wrap-content">
      <header class="header-page">
        <h1 class="page-title">Paper Review: DeepSeek-R1</h1>
        <div class="page-date"><span>2025, Jan 25&nbsp;&nbsp;&nbsp;&nbsp;</span></div>
      </header>
      <h2 id="overview"><strong>Overview</strong></h2>
<p>Recent work on Large Language Models (LLMs) has focused on <strong>reinforcement learning (RL) for self-improvement</strong>. While OpenAI’s o1 models enhanced <strong>reasoning abilities</strong> using inference-time scaling and chain-of-thought prompting, reproducing this performance in <strong>open-source models</strong> has remained a challenge.</p>

<p>DeepSeek-R1 explores <strong>pure RL-based reasoning</strong> using the <strong>GRPO algorithm</strong>, improving math and logic tasks <strong>without supervised fine-tuning (SFT)</strong>. It achieves:</p>
<ul>
  <li><strong>71.0% Pass@1 on AIME 2024</strong> (vs. 15.6% before RL, up to 86.7% with majority voting).</li>
  <li><strong>Performance parity with OpenAI-o1-0912 and o1-1217 in reasoning</strong>.</li>
  <li><strong>Superior distillation capabilities</strong>, improving smaller models such as Qwen2.5-32B and Llama.</li>
</ul>

<hr />

<h2 id="reinforcement-learning-algorithm-grpo"><strong>Reinforcement Learning Algorithm: GRPO</strong></h2>
<p>DeepSeek-R1 uses <strong>Group Relative Policy Optimization (GRPO)</strong>, a modification of <strong>PPO</strong> that eliminates the need for a critic network.</p>

<h3 id="policy-optimization-objective"><strong>Policy Optimization Objective</strong></h3>
<p>Given a set of outputs $ {o_1, o_2, \dots, o_G} $ sampled from the old policy $ \pi_{\theta_{\text{old}}} $, the GRPO objective is:
\(J_{\mathrm{GRPO}}(\theta) = \mathbb{E}_{q \sim P(Q),\, \{o_i\} \sim \pi_{\theta_{\text{old}}}} \left[
\frac{1}{G} \sum_{i=1}^{G} \min \left(
\frac{\pi_{\theta}(o_i \mid q)}{\pi_{\theta_{\text{old}}}(o_i \mid q)}\, A_i,\;
\operatorname{clip}\!\left( \frac{\pi_{\theta}(o_i \mid q)}{\pi_{\theta_{\text{old}}}(o_i \mid q)},\, 1-\epsilon,\, 1+\epsilon \right) A_i
\right)
\mathbin{-}\beta\, D_{\mathrm{KL}}\!\left(\pi_{\theta}\,\Vert\,\pi_{\text{ref}}\right)
\right]\)</p>

<p>where:</p>
<ul>
  <li>$ A_i $ is the <strong>advantage function</strong> based on group reward normalization:
\(A_i = \frac{r_i - \operatorname{mean}\bigl(\{r_1, \dots, r_G\}\bigr)}{\operatorname{std}\bigl(\{r_1, \dots, r_G\}\bigr)}\)</li>
  <li>$ D_{\mathrm{KL}}\left(\pi_{\theta}\,\Vert\,\pi_{\text{ref}}\right) $ <strong>regularizes policy updates</strong>, preventing collapse.</li>
</ul>

<hr />

<h2 id="reward-modeling"><strong>Reward Modeling</strong></h2>
<p>DeepSeek-R1 <strong>avoids reward hacking</strong> using structured, rule-based rewards:</p>
<ol>
  <li><strong>Accuracy Reward:</strong> Math/coding correctness is evaluated via test cases.</li>
  <li><strong>Format Reward:</strong> The model is required to produce its output in a structured format, ensuring a clear separation between its internal reasoning process and the final answer.</li>
</ol>

<hr />

<h2 id="emergent-self-evolution--benchmark-gains"><strong>Emergent Self-Evolution &amp; Benchmark Gains</strong></h2>
<p>During training, DeepSeek-R1 exhibits a self-evolution process by:</p>
<ul>
  <li>Increasing its reasoning steps and thinking time.</li>
  <li>Developing emergent behaviors such as self-reflection and multi-solution exploration.</li>
</ul>

<p>Key benchmark improvements include:</p>
<ul>
  <li><strong>AIME 2024 (Pass@1):</strong> Improvement from 15.6% to 71.0% (up to 86.7% with majority voting).</li>
  <li><strong>MATH-500 (Pass@1):</strong> Achieving 97.3%, comparable to leading models.</li>
  <li><strong>GPQA Diamond Tasks:</strong> Reaching 71.5% accuracy.</li>
</ul>

<hr />

<h2 id="multi-stage-training-strategy"><strong>Multi-Stage Training Strategy</strong></h2>
<p>DeepSeek-R1 utilizes a multi-stage training pipeline:</p>

<h3 id="cold-start-phase"><strong>Cold-Start Phase</strong></h3>
<p>A small amount of high-quality, long Chain-of-Thought (CoT) data is used to fine-tune the base model. This phase:</p>
<ul>
  <li>Enhances readability and output alignment.</li>
  <li>Structures responses in a format similar to:</li>
</ul>

<h3 id="reinforcement-learning-phase"><strong>Reinforcement Learning Phase</strong></h3>
<p>After the cold-start phase, iterative RL training is applied to further improve reasoning abilities, particularly in math and coding tasks. This phase leverages:</p>
<ul>
  <li><strong>Rule-based rewards</strong> for accuracy and language consistency.</li>
  <li>Incentives to ensure clear, structured chain-of-thought outputs.</li>
</ul>

<hr />

<h2 id="distillation-transferring-reasoning-to-smaller-models"><strong>Distillation: Transferring Reasoning to Smaller Models</strong></h2>
<p>DeepSeek-R1’s learned reasoning patterns are distilled into smaller dense models (e.g., Qwen2.5-32B, Llama variants). This process demonstrates that:</p>
<ul>
  <li>Distilled models outperform smaller models trained solely via RL.</li>
  <li>The reasoning capabilities of large models can be effectively transferred, preserving high performance.</li>
</ul>

<p>A comparison is provided below:</p>

<table>
  <thead>
    <tr>
      <th><strong>Model</strong></th>
      <th><strong>AIME 2024 Pass@1</strong></th>
      <th><strong>MATH-500 Pass@1</strong></th>
      <th><strong>GPQA Diamond Pass@1</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>RL on Qwen-32B</td>
      <td>47.0\%</td>
      <td>91.6\%</td>
      <td>55.0\%</td>
    </tr>
    <tr>
      <td><strong>Distilled Qwen-32B</strong></td>
      <td><strong>72.6\%</strong></td>
      <td><strong>94.3\%</strong></td>
      <td><strong>62.1\%</strong></td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="challenges-and-limitations"><strong>Challenges and Limitations</strong></h2>
<p>Key challenges identified include:</p>
<ul>
  <li><strong>Language Mixing:</strong> Occasional mixing of languages in outputs.</li>
  <li><strong>Prompt Sensitivity:</strong> Few-shot prompting can degrade performance.</li>
  <li><strong>Software Engineering Tasks:</strong> RL is less efficient for long-horizon tasks such as full-stack coding.</li>
</ul>

<hr />

<h2 id="conclusion-and-future-work"><strong>Conclusion and Future Work</strong></h2>
<p>DeepSeek-R1 demonstrates that LLMs can develop advanced reasoning capabilities through pure RL, without relying on supervised fine-tuning. The integrated pipeline of cold-start fine-tuning, iterative RL training, and subsequent distillation yields state-of-the-art performance on reasoning benchmarks.</p>

<p>Future research directions include:</p>
<ul>
  <li>Enhancing <strong>multilingual reasoning</strong> consistency.</li>
  <li>Developing more robust <strong>few-shot prompting</strong> strategies.</li>
  <li>Optimizing RL for <strong>long-horizon tasks</strong> in software engineering.</li>
</ul>


      <div class="page-footer">
        <div class="page-share">
          <a href="https://twitter.com/intent/tweet?text=Paper Review: DeepSeek-R1&url=http://localhost:4000/deepseek/"
            title="Share on Twitter" rel="nofollow" target="_blank">Twitter</a>
          <a href="https://facebook.com/sharer.php?u=http://localhost:4000/deepseek/" title="Share on Facebook"
            rel="nofollow" target="_blank">Facebook</a>
          <a href="https://plus.google.com/share?url=http://localhost:4000/deepseek/" title="Share on Google+"
            rel="nofollow" target="_blank">Google+</a>
        </div>
        <div class="page-tag">
          
          <a href="/tags#AI" class="tag">&#35; AI</a>
          
          <a href="/tags#Paper Review" class="tag">&#35; Paper Review</a>
          
          <a href="/tags#Reinforcement Learning" class="tag">&#35; Reinforcement Learning</a>
          
          <a href="/tags#Math" class="tag">&#35; Math</a>
          
        </div>
      </div>
      <section class="comment-area">
  <div class="comment-wrapper">
    
  </div>
</section> <!-- End Comment Area -->

    </div> <!-- End Wrap Content -->
  </div> <!-- End Page Content -->
</article> <!-- End Article Page -->
</div>

  </div>
  
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', '', 'auto');
  ga('send', 'pageview');
</script> <!-- End Analytics -->

</body>
</html>
