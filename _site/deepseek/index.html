<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>Paper Review: DeepSeek-R1 - Hello, world! I'm Eric Ji</title>

  <!-- Edit site and author settings in `_config.yml` to make the social details your own -->

    <meta content="Hello, world! I'm Eric Ji" property="og:site_name">
  
    <meta content="Paper Review: DeepSeek-R1" property="og:title">
  
  
    <meta content="article" property="og:type">
  
  
    <meta content="A review of DeepSeek-R1, an RL-trained reasoning LLM that improves self-evolution in problem-solving." property="og:description">
  
  
    <meta content="http://localhost:4000/deepseek/" property="og:url">
  
  
    <meta content="2025-01-25T21:00:20-05:00" property="article:published_time">
    <meta content="http://localhost:4000/about/" property="article:author">
  
  
    <meta content="http://localhost:4000/assets/img/deepseek.avif" property="og:image">
  
  
    
  
  
    
    <meta content="AI" property="article:tag">
    
    <meta content="Paper Review" property="article:tag">
    
    <meta content="Reinforcement Learning" property="article:tag">
    
    <meta content="Math" property="article:tag">
    
  

    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@">
    <meta name="twitter:creator" content="@">
  
    <meta name="twitter:title" content="Paper Review: DeepSeek-R1">
  
  
    <meta name="twitter:url" content="http://localhost:4000/deepseek/">
  
  
    <meta name="twitter:description" content="A review of DeepSeek-R1, an RL-trained reasoning LLM that improves self-evolution in problem-solving.">
  
  
    <meta name="twitter:image:src" content="http://localhost:4000/assets/img/deepseek.avif">
  

	<meta name="description" content="A review of DeepSeek-R1, an RL-trained reasoning LLM that improves self-evolution in problem-solving.">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta property="og:image" content="">
	<link rel="shortcut icon" href="/assets/img/favicon/favicon.ico" type="image/x-icon">
	<link rel="apple-touch-icon" href="/assets/img/favicon/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicon/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicon/apple-touch-icon-144x144.png">
	<!-- Chrome, Firefox OS and Opera -->
	<meta name="theme-color" content="#263959">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#263959">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#263959">
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=PT+Serif:400,700|Lato:300,400,700&display=swap" rel="stylesheet">
	<!-- Font Awesome -->
	<link rel="stylesheet" href="/assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="/assets/css/main.css">
</head>

<body>

  <div class="wrapper">
    <aside class="sidebar">
  <header>
    <div class="about">
      <div class="cover-author-image">
        <a href="/"><img src="/assets/img/eric-headshot.jpg" alt="Eric Ji"></a>
      </div>
      <div class="author-name">Eric Ji</div>
      <p>Feeling self important enough to share my opinions online</p>
    </div>
  </header> <!-- End Header -->
  <footer>
    <section class="contact">
      <h3 class="contact-title">Contact me</h3>
      <ul>
        
          <li><a href="https://twitter.com/artemsheludko_" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a></li>
        
        
          <li><a href="https://facebook.com/" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a></li>
        
        
          <li class="github"><a href="http://github.com/ej4592" target="_blank"><i class="fa fa-github"></i></a></li>
        
        
          <li class="linkedin"><a href="https://in.linkedin.com/" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a></li>
        
        
          <li class="email"><a href="mailto:ericji300@gmail.com"><i class="fa fa-envelope-o"></i></a></li>
        
      </ul>
    </section> <!-- End Section Contact -->
    <div class="copyright">
      <p>2025 &copy; Eric Ji</p>
    </div>
  </footer> <!-- End Footer -->
</aside> <!-- End Sidebar -->
<div class="content-box clearfix">
  
<!-- MathJax Configuration -->
<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<!-- MathJax Script -->
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<article class="article-page">
  <div class="page-content">
    
    <div class="page-cover-image">
      <figure>
        <img class="page-image" src="/assets/img/deepseek.avif"
          alt="Paper Review: DeepSeek-R1">
        
      </figure>
    </div> <!-- End Page Cover Image -->
    
    <div class="wrap-content">
      <header class="header-page">
        <h1 class="page-title">Paper Review: DeepSeek-R1</h1>
        <div class="page-date"><span>2025, Jan 25&nbsp;&nbsp;&nbsp;&nbsp;</span></div>
      </header>
      <h2 id="introduction"><strong>Introduction</strong></h2>
<p><strong>DeepSeek-R1</strong> is a fully open-source Large Language Model (LLM) that achieves advanced reasoning capabilities primarily through <strong>reinforcement learning (RL)</strong>—bypassing the heavy supervised-instruction tuning seen in many other models. Its key innovation hinges on a <strong>rule-based</strong> strategy called <strong>Group Relative Policy Optimization (GRPO)</strong>. This post explores the <strong>mathematical</strong> aspects of <strong>Transformers</strong> and <strong>GRPO</strong>, and details the <strong>training pipeline</strong> behind DeepSeek-R1’s impressive problem-solving performance.</p>

<hr />

<h2 id="recap-how-are-llms-traditionally-trained"><strong>Recap: How Are LLMs Traditionally Trained?</strong></h2>

<h3 id="1-transformer-architecture-intuition--math"><strong>1. Transformer Architecture: Intuition &amp; Math</strong></h3>

<p>Modern LLMs like DeepSeek typically follow the <strong>Transformer</strong> design from the seminal <em>“Attention Is All You Need”</em>. The Transformer is built upon <strong>attention mechanisms</strong>, not recurrence or convolution. Here’s how it works:</p>

<ol>
  <li><strong>Embedding &amp; Positional Encoding</strong>
    <ul>
      <li><strong>Embedding</strong>: Each token $( t )$ is mapped to a vector $( \mathbf{e}_t )$.</li>
      <li>
        <p><strong>Positional Encoding</strong>: Because Transformers handle tokens in parallel rather than sequentially, they need to encode token order separately. A popular approach is:</p>

        <p>\(\mathrm{PE}(\text{pos},\,2i) \;=\; \sin\!\Bigl(\frac{\text{pos}}{10000^{2i / d_{\text{model}}}}\Bigr), 
\quad
\mathrm{PE}(\text{pos},\,2i + 1) \;=\; \cos\!\Bigl(\frac{\text{pos}}{10000^{2i / d_{\text{model}}}}\Bigr),\)
where $(\text{pos})$ is the token position, $(i)$ indexes the embedding dimension, and $(d_{\text{model}})$ is the total embedding size.</p>
      </li>
    </ul>
  </li>
  <li><strong>Multi-Head Self-Attention (MHSA)</strong>
    <ul>
      <li>Each token embedding is projected into <strong>query</strong> $( Q )$, <strong>key</strong> $( K )$, and <strong>value</strong> $( V )$ vectors.</li>
      <li>For one attention head, we have:
\(\mathrm{Attention}(Q,\,K,\,V)
\;=\;
\mathrm{softmax}\!\Bigl(\tfrac{Q K^\top}{\sqrt{d_k}}\Bigr)\,V,\)
where $( d_k )$ is often $( d_{\text{model}} / \text{num_heads} )$.</li>
      <li><strong>Parallel Heads</strong>: Multiple heads run simultaneously, letting the model attend to different aspects of the sequence (e.g., grammar, semantics).</li>
    </ul>
  </li>
  <li><strong>Feedforward &amp; Residual Layers</strong>
    <ul>
      <li>After attention, each token embedding passes through a position-wise feedforward network:
\(\mathrm{FFN}(x) 
\;=\;
\sigma\!\bigl(x\,W_1 + b_1\bigr)\,W_2 + b_2,\)
where $(\sigma)$ can any non linear activation function</li>
      <li><strong>Residual connections</strong> and <strong>layer normalization</strong> help training converge more reliably. Multiple layers of attention + feedforward form the Transformer’s main body.</li>
    </ul>
  </li>
</ol>

<p><strong>Intuitive Picture</strong>:</p>
<ul>
  <li><strong>Attention</strong>: Each token “looks at” all other tokens to decide which ones matter.</li>
  <li><strong>Multi-head</strong>: The model can capture multiple relationships in parallel.</li>
  <li><strong>Residuals</strong>: Ensure stable gradients, essential for training large models.</li>
</ul>

<hr />

<h3 id="2-pre-training-with-maximum-likelihood"><strong>2. Pre-training with Maximum Likelihood</strong></h3>

<p>Once the Transformer is set up, it’s <strong>pre-trained</strong> on massive text corpora via next-token prediction. If $((x,y))$ is a context–target pair in dataset $(\mathcal{D})$, the goal is:
\(\max_{\theta}
\;\sum_{(x,\,y)\,\in\,\mathcal{D}}
\;\log P\!\bigl(y \,\mid\, x;\,\theta\bigr),\)
where:</p>
<ul>
  <li>$( x )$ is a partial sequence (context),</li>
  <li>$( y )$ is the <strong>next token</strong>,</li>
  <li>$( \theta )$ are the Transformer’s parameters (including embeddings, attention layers, etc.).</li>
</ul>

<p>This <strong>self-supervised</strong> stage imbues the model with broad language understanding, enabling it to handle diverse inputs.</p>

<hr />

<h3 id="3-supervised-fine-tuning-sft"><strong>3. Supervised Fine-Tuning (SFT)</strong></h3>

<p>Following pre-training, many LLMs undergo <strong>supervised fine-tuning</strong> on curated instruction–response or Q&amp;A datasets:
\(\max_{\theta}
\;\sum_{(x,\,y)\,\in\,\mathcal{D}_{\mathrm{SFT}}}
\;\log P\!\bigl(y \,\mid\, x;\,\theta\bigr).\)
Here, (\mathcal{D}_{\mathrm{SFT}}) is usually smaller but <strong>high-quality</strong>, containing explicit instructions or question–answer pairs. This step further shapes the model to produce <strong>human-aligned</strong> outputs, improving clarity or relevance.</p>

<hr />

<h3 id="4-reinforcement-learning"><strong>4. Reinforcement Learning</strong></h3>

<p>Finally, some LLMs leverage <strong>RL</strong> to polish their policy (\pi_\theta). In <strong>RL from Human Feedback (RLHF)</strong>, for instance, model outputs are ranked or scored (often by human annotators), providing reward signals for desired responses.</p>

<p>The typical pipeline is:</p>
<ol>
  <li><strong>Pre-train</strong> on unlabeled text.</li>
  <li><strong>Supervised Fine-Tune</strong> on instruction data.</li>
  <li><strong>RL</strong> (e.g., RLHF) to optimize for specific user preferences or correctness signals.</li>
</ol>

<hr />

<h2 id="deepseek-r1-group-relative-policy-optimization-grpo"><strong>DeepSeek-R1: Group Relative Policy Optimization (GRPO)</strong></h2>

<h3 id="motivation"><strong>Motivation</strong></h3>
<p>DeepSeek-R1 focuses on <strong>math and logical reasoning</strong> but avoids extensive supervised data. Instead, it employs <strong>GRPO</strong>, a variant of <strong>Proximal Policy Optimization (PPO)</strong> that <strong>removes</strong> the need for a critic network. By <strong>normalizing</strong> rewards within each mini-batch, GRPO emphasizes which outputs are best relative to peers.</p>
<h3 id="sampling-and-rewards"><strong>Sampling and Rewards</strong></h3>
<ul>
  <li>Let $q$ be the query (e.g., a math question).</li>
  <li>Let $\pi_{\theta_{\text{old}}}$ be the old policy parameters.
For each $q$, the model <strong>samples</strong> a group of outputs $({o_1,\dots,o_G})$, each including:
    <ol>
      <li><strong>Chain-of-thought</strong> (internal reasoning),</li>
      <li><strong>Final answer</strong>.
A rule-based reward function $(r_i)$ assigns scores based on:</li>
    </ol>
  </li>
  <li><strong>Accuracy</strong> (final answer correctness),</li>
  <li><strong>Format</strong> (proper <code class="language-plaintext highlighter-rouge">&lt;think&gt;</code> tags, clarity, structure).
    <h3 id="group-relative-advantage"><strong>Group-Relative Advantage</strong></h3>
    <p>While PPO typically uses a critic $V^\pi$, GRPO <strong>omits</strong> it in favor of a <strong>group-relative</strong> advantage:
\(A_i
\;=\;
\frac{\,r_i \,-\, \mathrm{mean}(\{r_1,\dots,r_G\})\,}
     {\,\mathrm{std}(\{r_1,\dots,r_G\})\,}.\)
This normalization shows how much better or worse each output (o_i) is compared to the <strong>mean</strong> of that batch.</p>
    <h3 id="grpo-objective"><strong>GRPO Objective</strong></h3>
    <p>The policy $(\theta)$ is <strong>updated</strong> by maximizing:
\(J_{\mathrm{GRPO}}(\theta)
=
\mathbb{E}_{\,q \,\sim\, P(Q),\;\{o_i\}\,\sim\, \pi_{\theta_{\text{old}}}\,}
\Biggl[
\frac{1}{G}\sum_{\,i=1}^{G}
  \min\!\Bigl(
    \frac{\,\pi_{\theta}(o_i \,\mid\, q)\,}
         {\pi_{\theta_{\text{old}}}(o_i \,\mid\, q)}\,A_i,\;\;
    \operatorname{clip}\!\Bigl(
      \tfrac{\pi_{\theta}(o_i \,\mid\, q)}
            {\pi_{\theta_{\text{old}}}(o_i \,\mid\, q)},
      1-\epsilon,\,
      1+\epsilon
    \Bigr)\,A_i
  )
\,-\, \beta\, D_{\mathrm{KL}}\!\bigl(\pi_{\theta}\,\Vert\, \pi_{\text{ref}}\bigr)
\Biggr],\)</p>
  </li>
</ul>

<p>where:</p>
<ul>
  <li>$\displaystyle \frac{\pi_{\theta}(o_i \mid q)}{\pi_{\theta_{\text{old}}}(o_i \mid q)}$ is the ratio of new vs. old policy probabilities,</li>
  <li>$\min(\cdot,\cdot)$ and $\operatorname{clip}(\cdot)$ follow PPO’s trust-region concept,</li>
  <li>$D_{\mathrm{KL}}!\bigl(\pi_{\theta}\,\Vert\,\pi_{\text{ref}}\bigr)$ is a penalty term limiting drastic policy shifts.</li>
</ul>

<hr />

<h2 id="deepseek-r1-training-pipeline"><strong>DeepSeek-R1 Training Pipeline</strong></h2>

<ol>
  <li><strong>Base Pre-Trained Model</strong>
    <ul>
      <li>Start with <strong>DeepSeek-V3-Base</strong>, which has already learned general language usage via next-token prediction.</li>
    </ul>
  </li>
  <li><strong>DeepSeek-R1-Zero (Pure RL)</strong>
    <ul>
      <li><strong>Skip</strong> large-scale supervised fine-tuning.</li>
      <li>Apply GRPO with <strong>rule-based</strong> rewards (accuracy, format).</li>
      <li>Test math/logic automatically for correctness.</li>
    </ul>
  </li>
  <li><strong>Cold-Start Refinement</strong>
    <ul>
      <li>Minimal supervised tuning on a small, hand-chosen dataset.</li>
      <li>Aims to fix issues like language mixing or unclear formatting.</li>
    </ul>
  </li>
  <li><strong>Further RL</strong>
    <ul>
      <li>More GRPO passes on math, logic, coding tasks.</li>
      <li>Possibly new rule-based signals (style, language constraints).</li>
    </ul>
  </li>
  <li><strong>Rejection Sampling + Additional Fine-Tuning</strong>
    <ul>
      <li>Generate a large set of outputs.</li>
      <li><strong>Filter</strong> out poor solutions.</li>
      <li>Fine-tune on the best ones to enhance overall quality.</li>
    </ul>
  </li>
  <li><strong>Diverse RL</strong>
    <ul>
      <li>Final RL run on a broader task range.</li>
      <li>Mix rule-based feedback (for clearly testable tasks) with model-based or partial human feedback for subjective questions.</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="emergent-reasoning--performance"><strong>Emergent Reasoning &amp; Performance</strong></h2>

<h3 id="self-evolution"><strong>Self-Evolution</strong></h3>
<p>With <strong>group-normalized</strong> rewards, DeepSeek-R1 <strong>spontaneously</strong> learns to:</p>
<ul>
  <li>Lengthen chain-of-thought for tough problems,</li>
  <li>Correct initial reasoning mid-solution,</li>
  <li>Demonstrate “Aha” moments of insight.</li>
</ul>

<h3 id="benchmark-gains"><strong>Benchmark Gains</strong></h3>
<ul>
  <li><strong>AIME</strong> (Pass@1) rose from about 15.6% (pre-RL) to over 70% using GRPO.</li>
  <li><strong>MATH</strong> tasks approached state-of-the-art accuracy, rivaling or surpassing some proprietary models.</li>
</ul>

<hr />

<h2 id="conclusion"><strong>Conclusion</strong></h2>
<p>DeepSeek-R1 exemplifies how LLMs can be refined <strong>purely via rule-based RL</strong>, largely <strong>without</strong> huge supervised instruction data. By harnessing <strong>group-relative</strong> advantage (GRPO) and a staged training pipeline, it narrows the gap between open-source and proprietary approaches—particularly in math/logic reasoning.</p>

<p>As LLM innovation continues, this strategy may <strong>reduce</strong> reliance on massive human annotations. With a reported $5 million training budget, DeepSeek-R1 illustrates that <strong>scalable RL</strong> for LLMs is both feasible and transformative for the future of AI research.</p>

      <div class="page-footer">
        <div class="page-share">
          <a href="https://twitter.com/intent/tweet?text=Paper Review: DeepSeek-R1&url=http://localhost:4000/deepseek/"
            title="Share on Twitter" rel="nofollow" target="_blank">Twitter</a>
          <a href="https://facebook.com/sharer.php?u=http://localhost:4000/deepseek/" title="Share on Facebook"
            rel="nofollow" target="_blank">Facebook</a>
          <a href="https://plus.google.com/share?url=http://localhost:4000/deepseek/" title="Share on Google+"
            rel="nofollow" target="_blank">Google+</a>
        </div>
        <div class="page-tag">
          
          <a href="/tags#AI" class="tag">&#35; AI</a>
          
          <a href="/tags#Paper Review" class="tag">&#35; Paper Review</a>
          
          <a href="/tags#Reinforcement Learning" class="tag">&#35; Reinforcement Learning</a>
          
          <a href="/tags#Math" class="tag">&#35; Math</a>
          
        </div>
      </div>
      <section class="comment-area">
  <div class="comment-wrapper">
    
  </div>
</section> <!-- End Comment Area -->

    </div> <!-- End Wrap Content -->
  </div> <!-- End Page Content -->
</article> <!-- End Article Page -->
</div>

  </div>
  
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', '', 'auto');
  ga('send', 'pageview');
</script> <!-- End Analytics -->

</body>
</html>
